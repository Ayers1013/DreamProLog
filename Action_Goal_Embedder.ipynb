{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf2bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import init_config\n",
    "from controller import Controller\n",
    "from dataset.process import TokenParser\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffc85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the DreamProlog algorithm.\n",
      "There was a problem with the provided arguments. The program will run in the default setting:\n",
      "--configs prolog --logdir logdir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --configs CONFIGS [CONFIGS ...] --logdir\n",
      "                             LOGDIR\n",
      "ipykernel_launcher.py: error: the following arguments are required: --configs, --logdir\n",
      "Exception ignored in: <function Controller.__del__ at 0x7f22de708c20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kristsz/DreamProLog/controller.py\", line 133, in __del__\n",
      "    for env in self.train_envs + self.eval_envs:\n",
      "  File \"/home/kristsz/DreamProLog/misc/autoconfig.py\", line 94, in __getattr__\n",
      "    return self[name]\n",
      "  File \"/home/kristsz/DreamProLog/misc/autoconfig.py\", line 101, in __getitem__\n",
      "    if name in node.__params:\n",
      "  File \"/home/kristsz/DreamProLog/misc/autoconfig.py\", line 91, in __getattr__\n",
      "    if name == '_ConfigurationNode__params': raise AttributeError('ConfiguredModule must be initialized. Just add super().__init__()')\n",
      "AttributeError: ConfiguredModule must be initialized. Just add super().__init__()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9673/1558684856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "ctrl = Controller(*init_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = ctrl.datasetManager._train_eps._meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for v in meta.values():\n",
    "    ds.extend([str(s)[1:] for s in v['action_space_text']])\n",
    "    \n",
    "parser = TokenParser()\n",
    "\n",
    "def pad(narr):\n",
    "    size = narr.size\n",
    "    return np.pad(narr, [0, 128-size])\n",
    "\n",
    "parsed_ds = []\n",
    "for dp in ds:\n",
    "    parsed_ds.append(pad(np.array(parser.parse(dp), dtype = np.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9423678",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(ds[i], parsed_ds[i], sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 32\n",
    "def generator():\n",
    "    while True:\n",
    "        x = tf.constant([parsed_ds[np.random.randint(len(parsed_ds))] for i in range(BATCH_SIZE)])\n",
    "        yield x, x\n",
    "sgn = tf.TensorSpec((BATCH_SIZE, 128), dtype=tf.int32)\n",
    "tf_ds = tf.data.Dataset.from_generator(generator, output_signature = (sgn, sgn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac11b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 32\n",
    "def generator():\n",
    "    for data in parsed_ds:\n",
    "        x = tf.constant(data)\n",
    "        yield x, x\n",
    "sgn = tf.TensorSpec((128), dtype=tf.int32)\n",
    "tf_ds = tf.data.Dataset.from_generator(generator, output_signature = (sgn, sgn))\n",
    "tf_ds = tf_ds.shuffle(300*BATCH_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "for p in parsed_ds:\n",
    "    m = max(m, len(p))\n",
    "print(m, len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_it = iter(tf_ds)\n",
    "next(ds_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import EncoderLayer, DecoderLayer, MultiHeadAttention\n",
    "\n",
    "def scaled_dot_product_attention2(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n",
    "    tf.print(output.shape)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, querry,rate):\n",
    "        super().__init__()\n",
    "        self.layers = [EncoderLayer(d_model, num_heads, dff, rate) for i in range(5)]\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        shape=(querry, d_model)\n",
    "        self.variable = tf.Variable(tf.random.uniform(\n",
    "            shape, minval=0, maxval=1, dtype=tf.dtypes.float32, seed=69, name=None))\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "                                    \n",
    "    def mha(self, x):\n",
    "        q, k = x, x\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        #v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        #v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        v = tf.reshape(self.variable, (1, -1, self.num_heads, self.depth))\n",
    "        v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention2(\n",
    "            q, k, v, None)\n",
    "        \n",
    "        print(scaled_attention.shape)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output\n",
    "        \n",
    "        \n",
    "    def call(self, x, training):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,training, None)\n",
    "        \n",
    "        print('2', x.shape)\n",
    "            \n",
    "        x = self.mha(x,x, self.variable, None)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, output_length, rate):\n",
    "        super().__init__()\n",
    "        self.layers = [DecoderLayer(d_model, num_heads, dff, rate) for i in range(5)]\n",
    "        \n",
    "        shape=(BATCH_SIZE, output_length, d_model)\n",
    "        self.variable = tf.Variable(tf.random.uniform(\n",
    "            shape, minval=0, maxval=1, dtype=tf.dtypes.float32, seed=69, name=None))\n",
    "        \n",
    "        \n",
    "    def call(self, x, training):\n",
    "        y = self.variable\n",
    "        for layer in self.layers:\n",
    "            y, _, _ = layer(y, x, training, None, None)\n",
    "            \n",
    "        return y\n",
    "        \n",
    "class Net(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_heads, dff):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, num_heads, dff, 8, 0.04)  \n",
    "        self.enc_embed = tf.keras.layers.Embedding(300, d_model)\n",
    "        self.decoder = Decoder(d_model, num_heads, dff, 128, 0.04)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(0.08)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(300, activation='relu', use_bias=False)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        x = self.enc_embed(x)\n",
    "        x = self.encoder(x, training)\n",
    "        print(x[0].shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.decoder(x, training)\n",
    "        \n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(tf.keras.losses.Loss):\n",
    "    def __init__(self, omega=0.1):\n",
    "        super().__init__()\n",
    "        self.omega = omega\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y = tf.nn.softmax(y_pred, axis=-1)\n",
    "        y_probs = tf.gather(y, y_true, axis=-1, batch_dims=2)\n",
    "        y_log_probs = tf.math.log(y_probs+0.001)\n",
    "        #loss = self.omega*y_log_probs + (1-self.omega)*tf.math.cumsum(y_log_probs, axis=-1)\n",
    "        loss = y_log_probs*tf.math.cumprod(0.97*tf.ones((32,128)), axis=-1)\n",
    "        loss = -tf.math.reduce_sum(loss)/(128*32)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8954c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_true = next(ds_it)\n",
    "encoder = Encoder(256, 8, 512, 8, 0.04)\n",
    "q, k = x, x\n",
    "batch_size = tf.shape(q)[0]\n",
    "print(q.shape)\n",
    "\n",
    "q = encoder.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "k = encoder.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "#v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "v = tf.reshape(encoder.variable, (1, -1, self.num_heads, self.depth))\n",
    "v = tf.transpose(v, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(256, 8, 512)\n",
    "x, y_true = next(ds_it)\n",
    "y_pred = model(x, False)\n",
    "loss1 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss2 = Loss()\n",
    "print(loss2(y_true, y_pred))\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss=Loss(), metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_ds, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x, y_true = next(ds_it)\n",
    "y_pred = model(x, False)\n",
    "y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "y_probs = tf.gather(y_pred, y_true, axis=-1, batch_dims=2)\n",
    "y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "print(x[:4], y_pred[:4], tf.cast(100*y_probs[:4], dtype=tf.int32), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Tensor(\n",
    "[[10  6 13 14  3  4 73 24 14  3 73 24 13  3  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]\n",
    " [22  6 13 39 24 14  3 38 24 14  3 46 24 14  3 37 24 14  3  4 34 24 14  3\n",
    "  22  6 17 39 24 14  3 22  6 19 39 24 14  3  4 10  6 19 49 21 14 17 13  3\n",
    "  48 21 14 19 17  3 48 21 14 19 13  3  4 48 21 14 16 33 19 13 17 14 13  3\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]\n",
    " [ 4 34 24 13  3  2  3  4 96 24  7  8  3  4 22  6  9 24 13 39 24  7  8  3\n",
    "  15  6 13 99 24  7  8  3 36 24 13  3 97 24 13  3 98  6 13  7  8  3  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]\n",
    " [29 24 13  3 10  6 13 26  6 14 17  3  4 15  6 58  6 19 44 13  3 15  6 58\n",
    "   6 19 45 14  3 15  6 58  6 45 44 17  3  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]], shape=(4, 128), dtype=int32)\n",
    "\n",
    "tf.Tensor(\n",
    "[[10  6 13 14  3 10 73 24  3  3 73 24 14  3  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]\n",
    " [22  6 13 39 24 14  3 38 24 14  3 46 24 14  3 37 24 14  3  4 34 24 14  3\n",
    "  22  6 17 39  6 14  3 22  6 19 39 24 14  3  4 10  6 19 49 21 14 14  6  3\n",
    "  48 21 14 19 17  3 48 21 14 14 13  3  4 48 21 16 16 13  0  0 14 14 14  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]\n",
    " [ 4  6 13  7  8  3  8  3  4  3  7  7  8 24  3  3  7  8  8  3  3  8  8 24\n",
    "  24  8  3 24 24 24 24 24 24  8 24  3 24 24 24  3  3  3  7  7  3  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]\n",
    " [10  6 13  3  6 14 13  3 29 24 17  3  4  6  6 58  6 19 44 13  4 15  6 58\n",
    "   6  6 45 14  3 15  6 58 14 45  6 19 17  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
    "   0  0  0  0  0  0  0  0]], shape=(4, 128), dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[[2,2],[1,0]],[[1,1],[4,8]]])\n",
    "b = tf.constant([[[1],[10]]])\n",
    "tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a5779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
